import fitz  # PyMuPDF
import re
import json
import logging
from pathlib import Path
from datetime import datetime
import argparse

# ========= æ—¥å¿—é…ç½® =========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ========= å¤šä¹¦ç±æ”¯æŒ =========
BOOKS = {
    "æ—¥æœ¬èœ¡çƒ›å›¾æŠ€æœ¯": {
        "pdf": "file/æ—¥æœ¬èœ¡çƒ›å›¾æŠ€æœ¯.pdf",
        "keywords": [
            "Kçº¿", "å½¢æ€", "ä¿¡å·", "æŒ‡æ ‡", "å‡çº¿", "ç§»åŠ¨å¹³å‡çº¿", "åè½¬", "çªç ´",
            "æ”¯æ’‘", "é˜»åŠ›", "æˆäº¤é‡", "ä¹°å…¥", "å–å‡º", "å¤´è‚©é¡¶", "å¤´è‚©åº•", "é”¤å­çº¿",
            "åæ²¡å½¢æ€", "é»„æ˜æ˜Ÿ", "æ—©æ™¨æ˜Ÿ", "åå­—æ˜Ÿ", "è·³ç©º", "ç¼ºå£", "MACD", "RSI",
            "å¸ƒæ—å¸¦", "CCI", "SAR", "OBV", "DMI", "ATR", "å®ä½“", "å½±çº¿",
            "é¡¶éƒ¨", "åº•éƒ¨", "è¶‹åŠ¿", "ä¸Šæ¶¨", "ä¸‹è·Œ"
        ],
        "source": "æ—¥æœ¬èœ¡çƒ›å›¾æŠ€æœ¯"
    },
    "é‡‘èå¸‚åœºæŠ€æœ¯åˆ†æ": {
        "pdf": "file/å¸‚åœºæŠ€æœ¯åˆ†æ.pdf",
        "keywords": [
            "æ”¯æ’‘", "é˜»åŠ›", "çªç ´", "åè½¬", "è¶‹åŠ¿çº¿", "æˆäº¤é‡", "éœ‡è¡", "åŠ¨èƒ½",
            "éœ‡è¡æŒ‡æ ‡", "å¸ƒæ—å¸¦", "Kçº¿", "æŠ€æœ¯æŒ‡æ ‡", "ç§»åŠ¨å¹³å‡çº¿", "é€šé“", "èƒŒç¦»",
            "MACD", "RSI", "OBV", "å½¢æ€", "ä¸Šå‡ä¸‰æ³•", "ä¸‹é™ä¸‰æ³•", "å›¾è¡¨å½¢æ€",
            "å¤´è‚©é¡¶", "åŒåº•", "ä¸‰è§’å½¢", "æ——å½¢", "æ¥”å½¢", "çŸ©å½¢", "ä¹°å…¥", "å–å‡º"
        ],
        "source": "é‡‘èå¸‚åœºæŠ€æœ¯åˆ†æ"
    }
}

# ========= å·¥å…·å‡½æ•° =========
def extract_chapter_title(text):
    patterns = [
        r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )(\s|$)",
        r"^CHAPTER\s+\d+"
    ]
    for pat in patterns:
        match = re.match(pat, text.strip(), re.IGNORECASE)
        if match:
            return text.strip()
    return None

def extract_section_title(text):
    if len(text.strip()) <= 50 and any(kw in text for kw in ["å½¢æ€", "æ¨¡å‹", "Kçº¿", "çº¿å›¾", "æŒ‡æ ‡", "è¶‹åŠ¿"]):
        return text.strip()
    return None

def extract_figure(text):
    match = re.search(r"(å›¾\d+(\.\d+)?|Figure\s+\d+(\.\d+)?)", text, re.IGNORECASE)
    return match.group(1) if match else None

def classify_type(text):
    if re.match(r".*?[æ˜¯ä¸ºæŒ‡ç§°å«å±äºæ„æˆ]+.*?ä¸€ç§.*?[å½¢æ€æ¨¡å‹ç»“æ„]?", text):
        return "å®šä¹‰"
    elif any(x in text for x in ["ä¹°å…¥", "å–å‡º", "ä¿¡å·", "å»ºè®®", "è§¦å‘", "ç­–ç•¥"]):
        return "äº¤æ˜“é€»è¾‘"
    elif extract_figure(text):
        return "å›¾æ³¨"
    elif len(text) <= 60 and any(kw in text for kw in ["å½¢æ€", "æ¨¡å‹", "Kçº¿", "æŒ‡æ ‡"]):
        return "æœ¯è¯­"
    else:
        return "è¯´æ˜"

def is_valid(text, keywords):
    if not (15 <= len(text) <= 500):
        return False
    if not any(kw in text for kw in keywords):
        return False
    if re.search(r"(å…è´£å£°æ˜|ç‰ˆæƒæ‰€æœ‰|å¾®ä¿¡|æ‰«ç |www\.|http|å¤§å­¦|å‡ºç‰ˆç¤¾|å›¾è¡¨æ¥æº|æŠ€æœ¯æ”¯æŒ)", text):
        return False
    if text.endswith("ï¼š") or len(text.split()) < 2:
        return False
    return True

def merge_lines(lines, max_len=120):
    paragraph = ""
    for line in lines:
        if paragraph:
            paragraph += " " + line.strip()
        else:
            paragraph = line.strip()
        if len(paragraph) >= max_len:
            yield paragraph
            paragraph = ""
    if paragraph:
        yield paragraph

# ========= ä¸»æå–é€»è¾‘ =========
def process_book(book_name, config, debug=False):
    pdf_path = Path(config["pdf"])
    if not pdf_path.exists():
        logging.warning(f"âŒ æ‰¾ä¸åˆ°PDFæ–‡ä»¶ï¼š{pdf_path}")
        return

    logging.info(f"ğŸ“˜ å¼€å§‹å¤„ç†: {pdf_path.name}")
    keywords = config["keywords"]
    source = config["source"]

    results = []
    buffer = []
    chapter = None
    section = None

    doc = fitz.open(pdf_path)
    for page in doc:
        page_number = page.number + 1
        blocks = page.get_text("blocks")
        lines = [line.strip() for block in blocks for line in block[4].split("\n") if line.strip()]

        for para in merge_lines(lines):
            chapter_title = extract_chapter_title(para)
            if chapter_title:
                chapter = chapter_title
                continue

            section_title = extract_section_title(para)
            if section_title:
                section = section_title
                continue

            if debug:
                logging.debug(f"ğŸ§ª Raw para: {para}")

            if not is_valid(para, keywords):
                if debug:
                    logging.debug(f"âŒ æ— æ•ˆæ®µè½: {para}")
                continue

            fig = extract_figure(para)
            ptype = classify_type(para)

            if debug:
                logging.debug(f"âœ… æœ‰æ•ˆæ®µè½: {para} | ç±»å‹: {ptype} | é¡µç : {page_number}")

            if ptype == "å›¾æ³¨":
                buffer.append({
                    "text": para,
                    "page": page_number,
                    "chapter": chapter,
                    "section_title": section,
                    "figure": fig,
                    "type": ptype,
                    "source": source
                })
                continue

            if buffer:
                fig_entry = buffer.pop()
                fig_entry.update({
                    "text_full": fig_entry["text"] + " " + para,
                    "from_figure_merge": True,
                    "context_window": [],
                    "page": page_number,
                    "chapter": chapter,
                    "section_title": section
                })
                results.append(fig_entry)
            else:
                entry = {
                    "text": para,
                    "page": page_number,
                    "chapter": chapter,
                    "section_title": section,
                    "figure": fig,
                    "type": ptype,
                    "context_window": [r["text"] for r in results[-2:]] if len(results) >= 2 else [],
                    "source": source
                }
                results.append(entry)

    output_path = Path("data") / f"{book_name}_term_et_action_input_v7.jsonl"
    with open(output_path, "w", encoding="utf-8") as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    logging.info(f"âœ… å†™å…¥ {len(results)} æ¡è®°å½•è‡³ï¼š{output_path}")

# ========= ä¸»å…¥å£ =========
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--debug", action="store_true", help="æ˜¯å¦å¼€å¯è°ƒè¯•æ—¥å¿—")
    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    for name, cfg in BOOKS.items():
        logging.info(f"ğŸ“– æ­£åœ¨å¤„ç†ä¹¦ç±ï¼š{name}")
        process_book(name, cfg, debug=args.debug)